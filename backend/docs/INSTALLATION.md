# üöÄ Guide d'Installation - UCL AI Agent

## üìã Table des mati√®res

1. [Pr√©requis](#pr√©requis)
2. [Installation rapide](#installation-rapide)
3. [Installation manuelle](#installation-manuelle)
4. [Configuration](#configuration)
5. [V√©rification](#v√©rification)
6. [D√©pannage](#d√©pannage)

---

## üéØ Pr√©requis

### Obligatoires

- **Node.js 20+** ‚Üí [T√©l√©charger](https://nodejs.org)
- **Docker** ‚Üí [T√©l√©charger](https://docs.docker.com/get-docker/)
- **Ollama** (LLM local gratuit) ‚Üí [T√©l√©charger](https://ollama.com)

### V√©rifier les installations

```bash
# Node.js
node --version  # doit afficher v20.x.x ou sup√©rieur

# Docker
docker --version

# Ollama
ollama --version
```

---

## ‚ö° Installation Rapide

### Linux / macOS

```bash
# 1. T√©l√©charger le projet
git clone <votre-repo>
cd ucl-ai-agent

# 2. Lancer le script d'installation
chmod +x setup.sh
./setup.sh

# 3. D√©marrer le serveur
cd backend
npm run dev
```

### Windows

```powershell
# 1. T√©l√©charger le projet
git clone <votre-repo>
cd ucl-ai-agent

# 2. Installer les d√©pendances
cd backend
npm install

# 3. Copier la config
copy .env.example .env

# 4. D√©marrer Docker
docker-compose up -d

# 5. D√©marrer le serveur
npm run dev
```

---

## üîß Installation Manuelle

### 1Ô∏è‚É£ Installer Ollama

#### Linux / macOS

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Windows

T√©l√©charger depuis [ollama.com/download](https://ollama.com/download)

#### T√©l√©charger le mod√®le IA

```bash
# Llama 3.1 (recommand√©, ~4GB)
ollama pull llama3.1

# Alternatives plus l√©g√®res:
ollama pull mistral      # ~4GB
ollama pull phi-3        # ~2GB
ollama pull gemma        # ~2GB

# V√©rifier
ollama list
```

### 2Ô∏è‚É£ Configurer Docker

#### D√©marrer les services

```bash
cd backend
docker-compose up -d
```

#### V√©rifier les services

```bash
docker-compose ps
```

Vous devriez voir :
- ‚úÖ `ucl-redis` (port 6379)
- ‚úÖ `ucl-qdrant` (ports 6333, 6334)
- ‚úÖ `ucl-mongodb` (port 27017)

### 3Ô∏è‚É£ Installer les d√©pendances Node.js

```bash
cd backend
npm install
```

### 4Ô∏è‚É£ Configurer l'environnement

```bash
# Copier le fichier .env
cp .env.example .env

# √âditer la configuration
nano .env  # ou vim, code, etc.
```

**Configuration minimale:**

```env
# MongoDB
MONGODB_URI=mongodb://localhost:27017/ucl-ai

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Qdrant
QDRANT_URL=http://localhost:6333

# Redis
REDIS_URL=redis://localhost:6379

# Features
AI_AGENT_ENABLED=true
AUTO_SCRAPING_ENABLED=true
```

### 5Ô∏è‚É£ D√©marrer le serveur

```bash
# Mode d√©veloppement (avec rechargement auto)
npm run dev

# Mode production
npm start
```

---

## ‚öôÔ∏è Configuration

### Variables d'environnement importantes

| Variable | Description | D√©faut |
|----------|-------------|--------|
| `PORT` | Port du serveur | `5000` |
| `OLLAMA_MODEL` | Mod√®le LLM √† utiliser | `llama3.1` |
| `SCRAPING_INTERVAL_MINUTES` | Fr√©quence du scraping | `60` |
| `AI_AGENT_ENABLED` | Activer l'agent IA | `true` |
| `AUTO_SCRAPING_ENABLED` | Scraping automatique | `true` |
| `LOG_LEVEL` | Niveau de logs | `info` |

### Modifier la fr√©quence de scraping

```env
# Toutes les 30 minutes
SCRAPING_INTERVAL_MINUTES=30

# Toutes les 2 heures
SCRAPING_INTERVAL_MINUTES=120
```

### Changer de mod√®le LLM

```bash
# T√©l√©charger un nouveau mod√®le
ollama pull mistral

# Modifier .env
OLLAMA_MODEL=mistral
```

---

## ‚úÖ V√©rification

### 1. V√©rifier l'API

```bash
curl http://localhost:5000/health
```

**R√©ponse attendue:**

```json
{
  "status": "healthy",
  "timestamp": "2024-...",
  "uptime": 123.456
}
```

### 2. V√©rifier l'agent IA

```bash
curl http://localhost:5000/api/ai/health
```

**R√©ponse attendue:**

```json
{
  "success": true,
  "data": {
    "status": "healthy",
    "ollama": true,
    "qdrant": true,
    "model": "llama3.1"
  }
}
```

### 3. Tester le chat IA

```bash
curl -X POST http://localhost:5000/api/ai/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Qui a gagn√© la Ligue des Champions 2023?"}'
```

### 4. V√©rifier les services Docker

```bash
# Status
docker-compose ps

# Logs Redis
docker-compose logs redis

# Logs Qdrant
docker-compose logs qdrant

# Tous les logs
docker-compose logs -f
```

---

## üîç D√©pannage

### ‚ùå Erreur: "Ollama connection failed"

**Solutions:**

```bash
# V√©rifier qu'Ollama tourne
ollama list

# D√©marrer Ollama (si pas d√©j√† en service)
ollama serve

# Tester la connexion
curl http://localhost:11434/api/version
```

### ‚ùå Erreur: "MongoDB connection failed"

**Solutions:**

```bash
# Red√©marrer MongoDB
docker-compose restart mongodb

# V√©rifier les logs
docker-compose logs mongodb

# Tester la connexion
mongosh mongodb://localhost:27017/ucl-ai
```

### ‚ùå Erreur: "Qdrant connection failed"

**Solutions:**

```bash
# Red√©marrer Qdrant
docker-compose restart qdrant

# V√©rifier les logs
docker-compose logs qdrant

# Tester l'API
curl http://localhost:6333/collections
```

### ‚ùå Erreur: "Redis connection failed"

**Solutions:**

```bash
# Red√©marrer Redis
docker-compose restart redis

# Tester la connexion
docker exec -it ucl-redis redis-cli ping
# Doit retourner: PONG
```

### ‚ùå Port d√©j√† utilis√©

```bash
# Trouver le processus utilisant le port 5000
lsof -i :5000  # Linux/Mac
netstat -ano | findstr :5000  # Windows

# Changer le port dans .env
PORT=5001
```

### ‚ùå Mod√®le Ollama introuvable

```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull llama3.1

# V√©rifier dans .env que OLLAMA_MODEL correspond
```

### üêõ Mode Debug

Activer les logs d√©taill√©s:

```env
LOG_LEVEL=debug
NODE_ENV=development
```

---

## üìä Commandes Utiles

### Docker

```bash
# D√©marrer tous les services
docker-compose up -d

# Arr√™ter tous les services
docker-compose down

# Voir les logs en temps r√©el
docker-compose logs -f

# Red√©marrer un service
docker-compose restart redis

# Nettoyer (‚ö†Ô∏è supprime les donn√©es)
docker-compose down -v
```

### Ollama

```bash
# Lister les mod√®les
ollama list

# T√©l√©charger un mod√®le
ollama pull <model-name>

# Supprimer un mod√®le
ollama rm <model-name>

# Tester un mod√®le
ollama run llama3.1 "Bonjour!"
```

### Backend

```bash
# D√©veloppement
npm run dev

# Production
npm start

# Tests (TODO)
npm test

# Scraping manuel
npm run scrape
```

---

## üéØ Prochaines √âtapes

Une fois l'installation r√©ussie:

1. ‚úÖ Consulter la [Documentation API](./API.md)
2. ‚úÖ Lire le [Guide de D√©veloppement](./DEVELOPMENT.md)
3. ‚úÖ Tester les endpoints avec Postman
4. ‚úÖ Cr√©er le frontend React

---

## üìû Support

En cas de probl√®me:

1. V√©rifier les [Issues GitHub](lien-vers-issues)
2. Consulter la [FAQ](./FAQ.md)
3. Cr√©er une nouvelle Issue avec:
   - Description du probl√®me
   - Logs d'erreur
   - Configuration (masquer les secrets!)
   - Environnement (OS, versions)

---

## üí∞ Rappel: Co√ªts = 0‚Ç¨

- ‚úÖ Ollama ‚Üí 100% gratuit, local
- ‚úÖ Qdrant ‚Üí Self-hosted gratuit
- ‚úÖ Redis ‚Üí Self-hosted gratuit
- ‚úÖ MongoDB ‚Üí Free tier ou local
- ‚úÖ Puppeteer ‚Üí Gratuit

**Aucun frais API externe!** üéâ
